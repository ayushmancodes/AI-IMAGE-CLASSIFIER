{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ca7defb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: data_fake_real\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = \"data_fake_real\"\n",
    "print(\"Loading dataset from: \" + dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35c42b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 files belonging to 2 classes.\n",
      "Found 20000 files belonging to 2 classes.\n",
      "Training Classes:\n",
      "['FAKE', 'REAL']\n",
      "Testing Classes:\n",
      "['FAKE', 'REAL']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "img_height = 32 \n",
    "img_width = 32\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Loading training data\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  dataset_dir + \"/train\",\n",
    "  seed=10,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "# Loading validation data\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  dataset_dir + \"/test\",\n",
    "  seed=10,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "# Checking if it is loaded properly\n",
    "print(\"Training Classes:\")\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)\n",
    "\n",
    "print(\"Testing Classes:\")\n",
    "class_names = val_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1106dd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 32, 32, 32)        128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 32, 32, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 16, 16, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 16, 16, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 16, 16, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 8, 8, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               2097408   \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2340929 (8.93 MB)\n",
      "Trainable params: 2339713 (8.93 MB)\n",
      "Non-trainable params: 1216 (4.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Building the architecture \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "model = Sequential()\n",
    "    \n",
    "# Block 1\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Block 2\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "# Final Classifier Block\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# from tensorflow.keras.models import load_model\n",
    "# model = load_model('9-6-train-0.95-5epoxhs.h5')\n",
    "\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "# Build the model so we can see a summary\n",
    "model.build(input_shape=(None, 32, 32, 3))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e6b9af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping x_labels \n",
    "(x_train, y_train) = next(iter(train_ds))\n",
    "(x_val, y_val) = next(iter(val_ds))\n",
    "tf.experimental.numpy.experimental_enable_numpy_behavior()\n",
    "x_train_reshaped = x_train.reshape(-1, 32, 32, 3)\n",
    "x_val_reshaped = x_val.reshape(-1, 32, 32, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19380f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 32, 3)\n",
      "(32,)\n",
      "(32, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_train_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08c5aa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/5\n",
      "3125/3125 [==============================] - 1823s 583ms/step - loss: 0.2424 - accuracy: 0.9031 - val_loss: 0.2171 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "3125/3125 [==============================] - 2004s 641ms/step - loss: 0.1796 - accuracy: 0.9301 - val_loss: 0.2709 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "3125/3125 [==============================] - 2087s 667ms/step - loss: 0.1637 - accuracy: 0.9369 - val_loss: 0.0479 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "3125/3125 [==============================] - 1822s 583ms/step - loss: 0.1452 - accuracy: 0.9440 - val_loss: 0.1348 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "3125/3125 [==============================] - 1941s 621ms/step - loss: 0.1298 - accuracy: 0.9505 - val_loss: 0.2182 - val_accuracy: 0.9062 - lr: 0.0010\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#Defining callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "callbacks_list = [early_stopping, reduce_lr]\n",
    "\n",
    "# Data augmentation\n",
    "data_generator = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    shear_range=0.2,\n",
    "    fill_mode='nearest'\n",
    "\n",
    ")\n",
    "\n",
    "train_generator = data_generator.flow(x_train_reshaped, y_train, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=(x_val_reshaped, y_val),\n",
    "  epochs=5,\n",
    "  callbacks=callbacks_list,\n",
    "  verbose=1\n",
    ")\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8b3dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('9-6-train-0.95-5epoxhs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b0d7b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 163s 259ms/step - loss: 0.1293 - accuracy: 0.9504\n",
      "Test loss:  0.12930308282375336\n",
      "Test accuracy: 0.9503999948501587\n"
     ]
    }
   ],
   "source": [
    "l, a = model.evaluate(val_ds, verbose=1)\n",
    "print(\"Test loss: \", l)\n",
    "print(\"Test accuracy:\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3a024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('13-06-test-0.93-5epochs.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
